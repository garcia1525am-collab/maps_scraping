import time
import pandas as pd
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
import re
import os
import uuid
from datetime import datetime
from database_manager import DatabaseManager, LocalPersistence
import threading
import signal
import sys

class GoogleMapsScraperEnhanced:
    def __init__(self, auto_save=True, mysql_config=None, session_id=None):
        """Inicializa el scraper con capacidades mejoradas de persistencia"""
        self.driver = None
        self.wait = None
        self.auto_save = auto_save
        self.session_id = session_id or str(uuid.uuid4())[:8]
        
        # Sistema de persistencia
        self.db_manager = None
        self.local_persistence = LocalPersistence()
        self.extracted_businesses = []
        self.search_history = []
        
        # Configurar base de datos si est√° disponible
        if mysql_config:
            self.db_manager = DatabaseManager(**mysql_config)
            if self.db_manager.connect():
                self.db_manager.create_tables()
                print(f"‚úÖ Sistema de base de datos MySQL activo")
            else:
                print("‚ö†Ô∏è MySQL no disponible, usando solo persistencia local")
                self.db_manager = None
        
        # Timer para auto-guardado peri√≥dico
        self.auto_save_timer = None
        if self.auto_save:
            self._start_auto_save_timer()
        
        self.setup_driver()
    
    def _signal_handler(self, signum, frame):
        """Maneja interrupciones del sistema para guardar datos"""
        print(f"\nüö® Interrupci√≥n detectada (se√±al {signum})")
        print("üíæ Guardando datos antes de cerrar...")
        
        self._save_current_session()
        self.close()
        
        print("‚úÖ Datos guardados. Cerrando aplicaci√≥n...")
        sys.exit(0)
    
    def _start_auto_save_timer(self):
        """Inicia timer para auto-guardado cada 2 minutos"""
        def auto_save():
            if self.extracted_businesses:
                print("üîÑ Auto-guardado ejecut√°ndose...")
                self._save_current_session()
            
            # Programar siguiente auto-guardado
            self.auto_save_timer = threading.Timer(120.0, auto_save)  # 2 minutos
            self.auto_save_timer.daemon = True
            self.auto_save_timer.start()
        
        self.auto_save_timer = threading.Timer(120.0, auto_save)
        self.auto_save_timer.daemon = True
        self.auto_save_timer.start()
        print("‚è∞ Auto-guardado activado (cada 2 minutos)")
    
    def _save_current_session(self):
        """Guarda la sesi√≥n actual en MySQL y localmente"""
        if not self.extracted_businesses and not self.search_history:
            return
        
        session_data = {
            'session_id': self.session_id,
            'extracted_businesses': self.extracted_businesses,
            'search_history': self.search_history,
            'timestamp': datetime.now().isoformat(),
            'total_businesses': len(self.extracted_businesses)
        }
        
        # Guardar en MySQL si est√° disponible
        if self.db_manager:
            try:
                # Guardar negocios en lotes si hay muchos nuevos
                new_businesses = [b for b in self.extracted_businesses if not b.get('saved_to_db', False)]
                if new_businesses:
                    saved_count = self.db_manager.save_businesses_batch(new_businesses)
                    # Marcar como guardados
                    for business in new_businesses:
                        business['saved_to_db'] = True
                    print(f"üíæ {saved_count} negocios nuevos guardados en MySQL")
                
                # Guardar respaldo de sesi√≥n
                self.db_manager.save_session_backup(self.session_id, session_data)
                
                # Guardar historial de b√∫squedas nuevas
                new_searches = [s for s in self.search_history if not s.get('saved_to_db', False)]
                for search in new_searches:
                    self.db_manager.save_search_history(search)
                    search['saved_to_db'] = True
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error guardando en MySQL: {e}")
        
        # Guardar localmente como respaldo
        self.local_persistence.save_session(session_data, self.session_id)
        
        # Guardar CSV de respaldo
        if self.extracted_businesses:
            self.local_persistence.save_csv_backup(
                self.extracted_businesses, 
                f"autosave_{self.session_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            )

    def load_previous_session(self, session_id=None):
        """Carga una sesi√≥n anterior"""
        target_session_id = session_id or self.session_id
        
        # Intentar cargar desde MySQL primero
        if self.db_manager:
            backup = self.db_manager.get_latest_session_backup(target_session_id)
            if backup:
                session_data = backup['datos']
                self.extracted_businesses = session_data.get('extracted_businesses', [])
                self.search_history = session_data.get('search_history', [])
                print(f"üìÇ Sesi√≥n {target_session_id} cargada desde MySQL")
                print(f"   üìä {len(self.extracted_businesses)} negocios recuperados")
                return True
        
        # Intentar cargar localmente
        session_data = self.local_persistence.load_latest_session(target_session_id)
        if session_data:
            self.extracted_businesses = session_data.get('extracted_businesses', [])
            self.search_history = session_data.get('search_history', [])
            print(f"üìÇ Sesi√≥n {target_session_id} cargada desde archivos locales")
            print(f"   üìä {len(self.extracted_businesses)} negocios recuperados")
            return True
        
        print(f"‚ÑπÔ∏è No se encontraron datos previos para la sesi√≥n {target_session_id}")
        return False

    def setup_driver(self):
        """Configura el navegador Chrome con undetected_chromedriver"""
        options = uc.ChromeOptions()
        
        # Configuraciones estables
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-plugins-discovery")
        options.add_argument("--disable-web-security")
        options.add_argument("--allow-running-insecure-content")
        options.add_argument("--no-first-run")
        options.add_argument("--disable-default-apps")
        
        # User-Agent m√°s realista
        options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        try:
            print("‚úÖ Configurando Undetected ChromeDriver...")
            
            # Crear directorio temporal para datos del usuario
            temp_user_data = os.path.join(os.getcwd(), f"temp_chrome_profile_{self.session_id}")
            options.add_argument(f"--user-data-dir={temp_user_data}")
            
            self.driver = uc.Chrome(
                options=options,
                version_main=None,
                driver_executable_path=None,
                use_subprocess=False
            )
            
            self.wait = WebDriverWait(self.driver, 25)
            print("‚úÖ Chrome iniciado correctamente")
            
        except Exception as e:
            print(f"‚ùå Error configurando Undetected ChromeDriver: {e}")
            print("\nüîÑ Intentando configuraci√≥n alternativa...")
            try:
                options = uc.ChromeOptions()
                options.add_argument("--no-sandbox")
                options.add_argument("--disable-dev-shm-usage")
                options.add_argument("--headless")
                
                self.driver = uc.Chrome(options=options)
                self.wait = WebDriverWait(self.driver, 25)
                print("‚úÖ Chrome iniciado en modo alternativo")
                
            except Exception as e2:
                print(f"‚ùå Error en configuraci√≥n alternativa: {e2}")
                raise

    def scroll_and_load_results(self, max_results=10):
        """Hace scroll inteligente para cargar m√°s resultados de Google Maps"""
        print(f"üîÑ Cargando hasta {max_results} resultados...")
        
        # Esperar un momento inicial para que cargue la p√°gina
        time.sleep(3)
        
        # Buscar el panel de resultados con selectores m√°s espec√≠ficos
        results_panel_selectors = [
            "div[role='main']",
            "div.m6QErb.DxyBCb.kA9KIf.dS8AEf",  # Panel lateral de resultados
            "div.Nv2PK.THOPZb",
            "div[data-value='Search results']",
            ".m6QErb",
            "[role='main'] [role='feed']"
        ]
        
        results_panel = None
        for selector in results_panel_selectors:
            try:
                results_panel = self.driver.find_element(By.CSS_SELECTOR, selector)
                if results_panel and results_panel.is_displayed():
                    print(f"‚úÖ Panel de resultados encontrado: {selector}")
                    break
            except:
                continue
        
        unique_urls = set()
        scroll_attempts = 0
        max_scroll_attempts = 20
        no_new_results_count = 0
        
        while len(unique_urls) < max_results and scroll_attempts < max_scroll_attempts:
            scroll_attempts += 1
            
            # Obtener enlaces actuales antes del scroll
            current_links = self.get_current_business_links()
            previous_count = len(unique_urls)
            
            # Agregar nuevos enlaces √∫nicos
            for link in current_links:
                if len(unique_urls) >= max_results:
                    break
                if link and '/maps/place/' in link:
                    unique_urls.add(link)
            
            current_count = len(unique_urls)
            print(f"   üìä Intento {scroll_attempts}: {current_count} resultados √∫nicos encontrados")
            
            # Auto-guardado cada 10 resultados nuevos
            if self.auto_save and current_count > 0 and current_count % 10 == 0:
                print("üîÑ Auto-guardado intermedio...")
                self._save_current_session()
            
            # Verificar si encontramos nuevos resultados
            if current_count == previous_count:
                no_new_results_count += 1
            else:
                no_new_results_count = 0
            
            # Si llevamos varios intentos sin nuevos resultados, salir
            if no_new_results_count >= 5:
                print(f"‚ö†Ô∏è No se encontraron nuevos resultados en los √∫ltimos {no_new_results_count} intentos")
                break
                
            if current_count >= max_results:
                print(f"‚úÖ ¬°Objetivo alcanzado! {current_count} resultados encontrados")
                break
            
            # Estrategias m√∫ltiples de scroll (igual que antes)
            try:
                if results_panel:
                    # M√©todo 1: Scroll en el panel de resultados
                    self.driver.execute_script("""
                        arguments[0].scrollBy(0, 800);
                        arguments[0].scrollTop = arguments[0].scrollTop;
                    """, results_panel)
                    
                    # M√©todo 2: Scroll hasta el √∫ltimo elemento visible
                    try:
                        last_result = results_panel.find_elements(By.CSS_SELECTOR, "a[href*='/maps/place/']")
                        if last_result:
                            self.driver.execute_script("arguments[0].scrollIntoView(true);", last_result[-1])
                    except:
                        pass
                        
                else:
                    # Scroll en toda la p√°gina si no encontramos el panel
                    self.driver.execute_script("window.scrollBy(0, 1000);")
                
                # M√©todos adicionales de scroll (similares al original)
                if scroll_attempts % 3 == 0:
                    actions = ActionChains(self.driver)
                    actions.send_keys(Keys.PAGE_DOWN).perform()
                    time.sleep(1)
                    actions.send_keys(Keys.END).perform()
                        
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error en scroll {scroll_attempts}: {e}")
            
            # Esperar a que se carguen nuevos resultados
            time.sleep(2.5)
        
        print(f"üèÅ Scroll completado: {len(unique_urls)} resultados √∫nicos disponibles")
        return list(unique_urls)

    def get_current_business_links(self):
        """Obtiene todos los enlaces de negocios visibles actualmente"""
        business_links = []
        
        # Selectores m√°s espec√≠ficos y completos
        selectors = [
            "a[href*='/maps/place/'][data-value]",
            "a[href*='/maps/place/']",
            "a[data-result-index]",
            ".hfpxzc",
            "a[jsaction*='navigate']",
            "div[role='article'] a[href*='place']",
            ".Nv2PK a[href*='place']",
            "[role='main'] a[href*='/maps/place/']",
            ".THOPZb a[href*='/maps/place/']"
        ]
        
        for selector in selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    try:
                        href = element.get_attribute('href')
                        if href and '/maps/place/' in href and href not in business_links:
                            if element.is_displayed():
                                business_links.append(href)
                    except:
                        continue
            except Exception as e:
                continue
        
        # Eliminar duplicados manteniendo el orden
        unique_links = []
        seen = set()
        for link in business_links:
            if link not in seen:
                seen.add(link)
                unique_links.append(link)
        
        return unique_links

    def search_businesses(self, url, max_results=10, search_name="busqueda"):
        """Busca y extrae informaci√≥n de negocios en Google Maps con auto-guardado"""
        start_time = time.time()
        print(f"üîç Accediendo a: {url}")
        
        if "google.com/maps" not in url and "maps.google.com" not in url:
            print("‚ùå La URL no parece ser una b√∫squeda v√°lida de Google Maps")
            return []
        
        try:
            self.driver.get(url)
            print("‚è≥ Esperando que cargue la p√°gina de resultados...")
            
            # Esperar a que aparezcan los primeros resultados
            time.sleep(5)
            
            # Intentar cerrar cualquier popup
            try:
                close_buttons = [
                    "button[aria-label*='close']",
                    "button[aria-label*='dismiss']", 
                    "button[data-value='Accept']",
                    ".VfPpkd-Bz112c-LgbsSe"
                ]
                for selector in close_buttons:
                    try:
                        button = self.driver.find_element(By.CSS_SELECTOR, selector)
                        if button.is_displayed():
                            button.click()
                            time.sleep(1)
                            break
                    except:
                        continue
            except:
                pass
            
            # Verificar resultados b√°sicos
            initial_selectors = [
                "a[href*='/maps/place/']",
                "div[role='article']",
                ".Nv2PK"
            ]
            
            found_results = False
            for selector in initial_selectors:
                try:
                    self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                    found_results = True
                    print(f"‚úÖ Resultados iniciales encontrados con: {selector}")
                    break
                except TimeoutException:
                    continue
            
            if not found_results:
                print("‚ùå No se encontraron resultados iniciales")
                return []
            
            # Scroll autom√°tico mejorado
            unique_urls = self.scroll_and_load_results(max_results)
            
            if not unique_urls:
                print("‚ùå No se pudieron obtener URLs de negocios")
                return []
            
            print(f"‚úÖ Se encontraron {len(unique_urls)} negocios √∫nicos para procesar")
            
            # Limitar a la cantidad solicitada
            urls_to_process = unique_urls[:max_results]
            
            businesses_data = []
            for i, business_url in enumerate(urls_to_process):
                print(f"\nüîç Procesando negocio {i+1}/{len(urls_to_process)}...")
                data = self.extract_business_data(business_url, i)
                if data:
                    # Agregar metadatos
                    data['busqueda'] = search_name
                    data['fecha_extraccion'] = datetime.now()
                    data['url_google_maps'] = business_url
                    data['session_id'] = self.session_id
                    
                    businesses_data.append(data)
                    self.extracted_businesses.append(data)
                    
                    # Auto-guardado cada 5 negocios
                    if self.auto_save and len(businesses_data) % 5 == 0:
                        print(f"üíæ Auto-guardado: {len(businesses_data)} negocios procesados")
                        self._save_current_session()
                
                # Pausa entre solicitudes
                time.sleep(2)
            
            # Guardar historial de b√∫squeda
            end_time = time.time()
            search_record = {
                'busqueda': search_name,
                'url': url,
                'resultados': len(businesses_data),
                'fecha': datetime.now(),
                'duracion_segundos': int(end_time - start_time),
                'parametros': {
                    'max_results': max_results,
                    'session_id': self.session_id
                }
            }
            self.search_history.append(search_record)
            
            # Guardado final
            if self.auto_save:
                print("üíæ Guardado final de la b√∫squeda...")
                self._save_current_session()
            
            return businesses_data
            
        except Exception as e:
            print(f"‚ùå Error durante la b√∫squeda: {e}")
            # Intentar guardar datos parciales en caso de error
            if self.auto_save and self.extracted_businesses:
                print("üíæ Guardando datos parciales debido al error...")
                self._save_current_session()
            return []

    def extract_business_data(self, url, index):
        """Navega a la p√°gina de un negocio y extrae toda su informaci√≥n"""
        business_data = {
            'indice': index, 
            'nombre': 'No disponible', 
            'calificacion': 'No disponible', 
            'num_reviews': 'No disponible', 
            'tipo': 'No disponible', 
            'direccion': 'No disponible', 
            'telefono': 'No disponible', 
            'website': 'No disponible', 
            'email': 'No disponible'
        }
        
        try:
            print(f"   üöó Navegando a la p√°gina del negocio...")
            self.driver.get(url)
            
            # Espera m√°s flexible para diferentes elementos
            selectors_to_wait = [
                "h1.DUwDvf",
                "h1[data-attrid='title']",
                ".x3AX1-LfntMc-header-title-title"
            ]
            
            title_element = None
            for selector in selectors_to_wait:
                try:
                    title_element = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                    break
                except TimeoutException:
                    continue
            
            if not title_element:
                print("   ‚ùå No se pudo cargar la p√°gina del negocio")
                return None
                
            print("   ‚úÖ P√°gina de detalles cargada.")

            # Extracci√≥n de datos con m√∫ltiples selectores de respaldo
            try:
                name_selectors = ["h1.DUwDvf", "h1[data-attrid='title']", ".x3AX1-LfntMc-header-title-title"]
                for selector in name_selectors:
                    try:
                        business_data['nombre'] = self.driver.find_element(By.CSS_SELECTOR, selector).text
                        break
                    except:
                        continue
            except: 
                pass
                
            try:
                rating_selectors = ["div.F7nice", ".MW4etd", ".ceNzKf"]
                for selector in rating_selectors:
                    try:
                        rating_text = self.driver.find_element(By.CSS_SELECTOR, selector).text
                        parts = rating_text.split('(')
                        if len(parts) > 0: 
                            business_data['calificacion'] = parts[0].strip()
                        if len(parts) > 1: 
                            business_data['num_reviews'] = parts[1].replace(')', '').strip()
                        break
                    except:
                        continue
            except: 
                pass
                
            try:
                type_selectors = ["button.DkEaL", ".YhemCb"]
                for selector in type_selectors:
                    try:
                        business_data['tipo'] = self.driver.find_element(By.CSS_SELECTOR, selector).text
                        break
                    except:
                        continue
            except: 
                pass
                
            try:
                address_selectors = [
                    "button[data-item-id='address']",
                    "[data-item-id='address'] .Io6YTe",
                    ".LrzXr"
                ]
                for selector in address_selectors:
                    try:
                        element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        address = element.get_attribute('aria-label') or element.text
                        business_data['direccion'] = address.replace('Direcci√≥n:', '').strip()
                        break
                    except:
                        continue
            except: 
                pass
                
            try:
                phone_selectors = [
                    "button[data-item-id^='phone:tel:']",
                    "[data-item-id*='phone'] .Io6YTe"
                ]
                for selector in phone_selectors:
                    try:
                        element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        phone = element.get_attribute('aria-label') or element.text
                        business_data['telefono'] = phone.replace('Tel√©fono:', '').strip()
                        break
                    except:
                        continue
            except: 
                pass
                
            try:
                website_selectors = [
                    "a[data-item-id='authority']",
                    "a[href^='http']:not([href*='google.com'])"
                ]
                for selector in website_selectors:
                    try:
                        element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        business_data['website'] = element.get_attribute('href')
                        break
                    except:
                        continue
            except: 
                pass
            
            print(f"   ‚úÖ Extra√≠do: {business_data['nombre']}")
            return business_data

        except TimeoutException:
            print("   ‚ùå La p√°gina del negocio no carg√≥ a tiempo")
            return None
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error inesperado extrayendo datos: {e}")
            return None

    def get_session_summary(self):
        """Obtiene resumen de la sesi√≥n actual"""
        return {
            'session_id': self.session_id,
            'total_businesses': len(self.extracted_businesses),
            'total_searches': len(self.search_history),
            'last_activity': datetime.now().isoformat() if self.extracted_businesses else None,
            'searches': [s['busqueda'] for s in self.search_history]
        }

    def export_session_data(self, format='csv', filename=None):
        """Exporta todos los datos de la sesi√≥n"""
        if not self.extracted_businesses:
            print("‚ùå No hay datos para exportar")
            return None
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"session_{self.session_id}_{timestamp}"
        
        if format.lower() == 'csv':
            filepath = f"{filename}.csv"
            df = pd.DataFrame(self.extracted_businesses)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"üìä Datos exportados a {filepath}")
            return filepath
        
        return None

    def save_to_csv(self, businesses, filename='negocios_extraidos.csv'):
        """M√©todo de compatibilidad con el scraper original"""
        if not businesses:
            print("‚ùå No hay datos para guardar")
            return
        
        df = pd.DataFrame(businesses)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nüíæ Datos guardados en {filename}")
        print(f"üìä Total de negocios extra√≠dos: {len(businesses)}")
        
        # Mostrar resumen de datos extra√≠dos
        print("\nüìã Resumen de extracci√≥n:")
        for col in df.columns:
            no_disponible = (df[col] == 'No disponible').sum()
            disponible = len(df) - no_disponible
            print(f"  {col}: {disponible}/{len(df)} disponibles")
    
    def close(self):
        """Cierra el navegador y limpia recursos"""
        # Cancelar timer de auto-guardado
        if self.auto_save_timer:
            self.auto_save_timer.cancel()
        
        # Guardado final
        if self.auto_save and (self.extracted_businesses or self.search_history):
            print("üíæ Guardado final antes de cerrar...")
            self._save_current_session()
        
        # Cerrar navegador
        if self.driver:
            self.driver.quit()
            print("\nüîí Navegador cerrado")
        
        # Cerrar conexi√≥n a base de datos
        if self.db_manager:
            self.db_manager.close()
        
        # Limpiar directorio temporal
        temp_user_data = os.path.join(os.getcwd(), f"temp_chrome_profile_{self.session_id}")
        try:
            if os.path.exists(temp_user_data):
                import shutil
                shutil.rmtree(temp_user_data, ignore_errors=True)
        except:
            pass
        
        print(f"‚úÖ Sesi√≥n {self.session_id} cerrada correctamente")


def main():
    print("üöÄ Google Maps Business Scraper MEJORADO con Persistencia")
    print("="*70)
    
    # Configuraci√≥n MySQL (opcional)
    mysql_config = None
    use_mysql = input("¬øUsar base de datos MySQL? (s/n): ").strip().lower()
    
    if use_mysql in ['s', 'si', 's√≠', 'y', 'yes']:
        print("\n‚öôÔ∏è Configuraci√≥n MySQL:")
        host = input("Host (localhost): ").strip() or "localhost"
        database = input("Base de datos (google_maps_scraper): ").strip() or "google_maps_scraper"
        user = input("Usuario (root): ").strip() or "root"
        password = input("Contrase√±a: ").strip()
        
        mysql_config = {
            'host': host,
            'database': database,
            'user': user,
            'password': password
        }
    
    # Crear scraper
    scraper = None
    session_id = input("\nID de sesi√≥n (Enter para nuevo): ").strip() or None
    
    try:
        scraper = GoogleMapsScraperEnhanced(
            auto_save=True,
            mysql_config=mysql_config,
            session_id=session_id
        )
        
        # Intentar cargar sesi√≥n anterior
        if session_id:
            scraper.load_previous_session()
        
        # Mostrar resumen de sesi√≥n
        summary = scraper.get_session_summary()
        print(f"\nüìä SESI√ìN ACTUAL: {summary['session_id']}")
        print(f"   Negocios: {summary['total_businesses']}")
        print(f"   B√∫squedas: {summary['total_searches']}")
        
        all_businesses = scraper.extracted_businesses.copy()
        search_count = len(scraper.search_history)
        
        while True:
            search_count += 1
            print(f"\nüîç B√öSQUEDA #{search_count}")
            print("-" * 30)
            
            # Solicitar URL
            url = input("üåê URL de b√∫squeda de Google Maps (o 'salir'): ").strip()
            
            if url.lower() in ['salir', 'exit', 'quit', 's', '']:
                break
            
            if "google.com/maps" not in url and "maps.google.com" not in url:
                print("‚ùå URL no v√°lida")
                continue
            
            # Par√°metros de b√∫squeda
            try:
                max_results = int(input("üìä ¬øCu√°ntos negocios extraer? (10): ") or "10")
            except:
                max_results = 10
            
            search_name = input("üè∑Ô∏è Nombre para esta b√∫squeda: ").strip()
            if not search_name:
                search_name = f"busqueda_{search_count}"
            
            print(f"\n‚ö° Procesando b√∫squeda: {search_name}")
            print("="*60)
            
            # Realizar b√∫squeda
            businesses = scraper.search_businesses(url, max_results=max_results, search_name=search_name)
            
            if businesses:
                print(f"‚úÖ B√∫squeda '{search_name}' completada: {len(businesses)} negocios")
            else:
                print(f"‚ùå No se obtuvieron resultados para '{search_name}'")
            
            # Resumen actual
            current_summary = scraper.get_session_summary()
            print(f"\nüìà RESUMEN ACTUAL:")
            print(f"   ‚Ä¢ Total negocios: {current_summary['total_businesses']}")
            print(f"   ‚Ä¢ Total b√∫squedas: {current_summary['total_searches']}")
            
            # Preguntar si continuar
            continuar = input(f"\nüîÑ ¬øHacer otra b√∫squeda? (s/n): ").strip().lower()
            if continuar not in ['s', 'si', 's√≠', 'y', 'yes']:
                break
        
        # Exportar datos finales
        if scraper.extracted_businesses:
            print(f"\nüíæ EXPORTANDO DATOS FINALES...")
            
            # CSV consolidado
            filepath = scraper.export_session_data('csv')
            
            # Mostrar resumen final
            final_summary = scraper.get_session_summary()
            print(f"\nüìä RESUMEN FINAL:")
            print(f"   ‚Ä¢ Sesi√≥n: {final_summary['session_id']}")
            print(f"   ‚Ä¢ Total negocios: {final_summary['total_businesses']}")
            print(f"   ‚Ä¢ Total b√∫squedas: {final_summary['total_searches']}")
            print(f"   ‚Ä¢ Archivo CSV: {filepath}")
            
            if mysql_config:
                print(f"   ‚Ä¢ Datos tambi√©n guardados en MySQL")
                
        else:
            print("\n‚ùå No se obtuvieron datos.")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Proceso interrumpido por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error fatal: {e}")
    
    finally:
        if scraper:
            scraper.close()
        print(f"\n‚úÖ ¬°Todos los datos han sido guardados autom√°ticamente!")
        print("üí° Puedes recuperar tu sesi√≥n la pr√≥xima vez usando el mismo ID")

if __name__ == "__main__":
    main()